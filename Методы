PCA — это алгоритм уменьшения размерности (dimensionality reduction). Он преобразует большое количество коррелированных признаков в меньшее количество некоррелированных, называемых главными компонентами.
Как работает?
PCA находит новые оси (главные компоненты) в данных, которые:
Максимизируют дисперсию данных вдоль себя (первая компонента).
Ортогональны (независимы) друг другу.
Первая главная компонента — это направление наибольшей изменчивости данных. Вторая — направление наибольшей изменчивости, перпендикулярное первой, и так далее.
Отбрасывая компоненты с наименьшей дисперсией, мы сохраняем самую важную информацию о данных, но в пространстве меньшей размерности.

Когда использовать?
Визуализация многомерных данных (проецируя на 2D или 3D).
Сжатие данных (уменьшение объема при минимальных потерях).
Ускорение других алгоритмов обучения (уменьшая количество признаков).
Борьба с мультиколлинеарностью в признаках.
Аналогия: Представьте тень от облака сложной формы. Тень на стене (2D) — это его упрощенная проекция, которая сохраняет общую форму, но теряет информацию о глубине. PCA делает нечто подобное, но математически оптимальным способом.
1.	Метод главных компонент (PCA) 
Нам подходит он потому что:
Почему:
У нас всего 2 признака (ride distance, booking value)
PCA поможет понять структуру взаимосвязи между ними
Можно визуализировать в 2D пространстве
Что даст:
Покажет, насколько сильно связаны расстояние и стоимость
Выявит основные направления изменчивости в данных
Поможет обнаружить аномалии и выбросы
